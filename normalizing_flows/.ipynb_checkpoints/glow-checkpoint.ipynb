{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_NUM_THREADS=8\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "%env OMP_NUM_THREADS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, glob\n",
    "sys.path += ['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from flows.glow import GlowFlow\n",
    "from flows.invert import Invert\n",
    "from flows.glow.affine_coupling import coupling_nn_glow\n",
    "from flows.glow.gaussianize import gaussianize\n",
    "from models.flvm import FlowLVM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from matplotlib.colors import LogNorm, SymLogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dir = '/home/cluster/hlasco/bulk1/GAN_dataset/'\n",
    "fList_t  = glob.glob(dset_dir+'/train/*/processed_data/snapshot.h5')\n",
    "fList_v  = glob.glob(dset_dir+'/validation/*/processed_data/snapshot.h5')\n",
    "nFiles_t = len(fList_t)\n",
    "\n",
    "ps_h = 64\n",
    "ps_l = ps_h//4\n",
    "\n",
    "# read .h5 file\n",
    "def readH5(f, n_channels=4):\n",
    "    f = f.numpy()\n",
    "    idx = np.random.randint(0,4)\n",
    "    idy = np.random.randint(0,4)\n",
    "    idz = np.random.randint(0,4)\n",
    "    with h5py.File(f, 'r') as fi:\n",
    "        u_hr = np.array(fi['HR/ux'][idx*ps_h:(idx+1)*ps_h,idy*ps_h:(idy+1)*ps_h,idz*ps_h:(idz+1)*ps_h], dtype=np.float32).reshape(ps_h,ps_h,ps_h,1)\n",
    "        u_lr = np.array(fi['LR/ux'][idx*ps_l:(idx+1)*ps_l,idy*ps_l:(idy+1)*ps_l,idz*ps_l:(idz+1)*ps_l], dtype=np.float32).reshape(ps_l,ps_l,ps_l,1)\n",
    "        v_hr = np.array(fi['HR/uy'][idx*ps_h:(idx+1)*ps_h,idy*ps_h:(idy+1)*ps_h,idz*ps_h:(idz+1)*ps_h], dtype=np.float32).reshape(ps_h,ps_h,ps_h,1)\n",
    "        v_lr = np.array(fi['LR/uy'][idx*ps_l:(idx+1)*ps_l,idy*ps_l:(idy+1)*ps_l,idz*ps_l:(idz+1)*ps_l], dtype=np.float32).reshape(ps_l,ps_l,ps_l,1)\n",
    "        w_hr = np.array(fi['HR/uz'][idx*ps_h:(idx+1)*ps_h,idy*ps_h:(idy+1)*ps_h,idz*ps_h:(idz+1)*ps_h], dtype=np.float32).reshape(ps_h,ps_h,ps_h,1)\n",
    "        w_lr = np.array(fi['LR/uz'][idx*ps_l:(idx+1)*ps_l,idy*ps_l:(idy+1)*ps_l,idz*ps_l:(idz+1)*ps_l], dtype=np.float32).reshape(ps_l,ps_l,ps_l,1)\n",
    "        r_hr = np.array(fi['HR/rho'][idx*ps_h:(idx+1)*ps_h,idy*ps_h:(idy+1)*ps_h,idz*ps_h:(idz+1)*ps_h], dtype=np.float32).reshape(ps_h,ps_h,ps_h,1)\n",
    "        r_lr = np.array(fi['LR/rho'][idx*ps_l:(idx+1)*ps_l,idy*ps_l:(idy+1)*ps_l,idz*ps_l:(idz+1)*ps_l], dtype=np.float32).reshape(ps_l,ps_l,ps_l,1)\n",
    "\n",
    "        m_vel = np.mean([u_lr, v_lr, w_lr])\n",
    "        s_vel = np.std([u_lr, v_lr, w_lr])\n",
    "\n",
    "        r_hr = np.log10(r_hr)\n",
    "        r_lr = np.log10(r_lr)\n",
    "\n",
    "        m_r = np.mean(r_lr)\n",
    "        s_r = np.std(r_lr)\n",
    "\n",
    "        u_hr = (u_hr - np.mean(u_lr))/np.std(u_lr)\n",
    "        u_lr = (u_lr - np.mean(u_lr))/np.std(u_lr)\n",
    "\n",
    "        v_hr = (v_hr - np.mean(v_lr))/np.std(v_lr)\n",
    "        v_lr = (v_lr - np.mean(v_lr))/np.std(v_lr)\n",
    "\n",
    "        w_hr = (w_hr - np.mean(w_lr))/np.std(w_lr)\n",
    "        w_lr = (w_lr - np.mean(w_lr))/np.std(w_lr)\n",
    "\n",
    "        r_hr = (r_hr - m_r)/s_r\n",
    "        r_lr = (r_lr - m_r)/s_r\n",
    "\n",
    "        ret_lr = np.concatenate((u_lr, v_lr, w_lr, r_lr), axis=-1)\n",
    "        if n_channels==4:\n",
    "            ret_hr = np.concatenate((u_hr, v_hr, w_hr, r_hr), axis=-1)\n",
    "        elif n_channels==3:\n",
    "            ret_hr = np.concatenate((u_hr, v_hr, w_hr), axis=-1)\n",
    "        elif n_channels==1:\n",
    "            ret_hr = r_hr\n",
    "        return ret_lr, ret_hr \n",
    "\n",
    "\n",
    "\n",
    "def readH5_wrapper(filename, n_channels=4):\n",
    "    # Assuming your data and labels are float32\n",
    "    # Your input is parse_function, who arg is filename, and you get X and y as output\n",
    "    # whose datatypes are indicated by the tuple argument\n",
    "    lr, hr = tf.py_function(readH5, [filename, n_channels], (tf.float32, tf.float32))\n",
    "    return hr, lr\n",
    "\n",
    "batch_size = 1\n",
    "n_channels = 1\n",
    "# Create dataset of filenames.\n",
    "X_train_ds = tf.data.Dataset.from_tensor_slices(fList_t)\n",
    "X_train_ds = X_train_ds.map(lambda x: readH5_wrapper(x, n_channels=n_channels))\n",
    "X_train_ds = X_train_ds.batch(batch_size).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(X_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 16, 16, 4) (1, 64, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "hr, lr = iterator.get_next()\n",
    "print(lr.shape, hr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Invert' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-399f065c2d17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mopt_flow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m model_8_128_rho = FlowLVM(glow, prior,\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mnum_bins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0minput_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/net/cephfs/home/hlasco/sr_turbulence/normalizing_flows/models/flvm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transform, prior, input_channels, num_bins, cond_fn, optimizer_flow, optimizer_cond, clip_grads, name)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_bins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Invert' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "n_channels=4\n",
    "kwargs_nn={'dim':3, 'min_filters':8, 'max_filters':128, 'num_blocks':0}\n",
    "\n",
    "affine_coupling = coupling_nn_glow(**kwargs_nn)\n",
    "inpt_shape = tf.TensorShape((None,None,None,None,n_channels))\n",
    "\n",
    "glow = Invert(GlowFlow(\n",
    "                   dim=3,\n",
    "                   upfactor=2,\n",
    "                   num_layers=3,\n",
    "                   depth=16,\n",
    "                   coupling_nn_ctor=affine_coupling))\n",
    "\n",
    "learning_rate = 2.0e-4\n",
    "prior = tfp.distributions.Normal(loc=0.0, scale=1.0)\n",
    "opt_flow = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model_8_128_rho = FlowLVM(glow, prior,\n",
    "                dim=3,\n",
    "                num_bins=16,\n",
    "                input_channels=n_channels,\n",
    "                optimizer_flow=opt_flow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(24938528, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(model_8_128_rho.param_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, None, None, None, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.TensorShape((None,*[None for i in range(dim)],n_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
